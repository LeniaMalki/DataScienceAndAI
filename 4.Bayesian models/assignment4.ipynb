{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DAT405 Introduction to Data Science and AI\n",
    "# Assignment 4: Spam classification using Naïve Bayes\n",
    "\n",
    "Student name | Hours spent on the tasks\n",
    "------------ | -------------\n",
    "Lenia Malki | 10\n",
    "Maële Belmont | 10\n",
    "\n",
    "ask:\n",
    "- question 1: open 1 mail or all of them? store them in lists or dataframe? why can't open file number 2 in easy_ham and file number 4 in hard_ham? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "Python modules need to be loaded to solve the tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "#Import scikit-learn dataset library\n",
    "from sklearn import datasets\n",
    "\n",
    "# Import train_test_split function\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Import Gaussian Naive Bayes model\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "There will be an overall grade for this assignment. To get a pass grade (grade 3), you need to pass \n",
    "items 1-3 below. To receive higher grades, finish items 4 and 5 as well.<br>\n",
    "In this assignment you will implement a Naïve Bayes classifier in Python that will classify emails into \n",
    "spam and non-spam (“ham”) classes. Your program should be able to train on a given set of spam \n",
    "and “ham” datasets.   You will work with the datasets available at https://spamassassin.apache.org/old/publiccorpus/. \n",
    "There are three types of files in this location: \n",
    "1. easy-ham: non-spam messages typically quite easy to differentiate from spam messages. \n",
    "2. hard-ham: non-spam messages more difficult to differentiate\n",
    "3. spam: spam messages\n",
    "\n",
    "Read the “readme.html” for a full description of the file contents. The .bz2-file can be unzipped using \n",
    "the linux command\n",
    "tar –xjvf file.bz2\n",
    "which will result in a directory named “file”, containing all email messages as separate files. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 - Preprocessing: \n",
    "#### a. Note that the email files contain a lot of extra information, besides the actual message. Ignore that for now and run on the entire text. Further down (in the higher grade part), you will be asked to filter out the headers and footers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. We don’t want to train and test on the same data. Split the spam and the ham datasets in a training set and a test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Method which saves files of a directory to a dataframe\n",
    "def getFiles(folderpath):\n",
    "    filepaths  = [os.path.join(folderpath, name) for name in os.listdir(folderpath)]\n",
    "    df = pd.DataFrame(filepaths)\n",
    "    return df\n",
    "\n",
    "#Load dataset to each variable\n",
    "e_ham_files = getFiles('easy_ham')\n",
    "h_ham_files = getFiles('hard_ham') \n",
    "spam_ham_files = getFiles('spam')\n",
    "\n",
    "#Merging ham files together as they are of the same type and keeping the amount of variables low\n",
    "ham_files = pd.concat([e_ham_files, h_ham_files])\n",
    "\n",
    "# Split dataset into training set and test set (70-30)\n",
    "ham_train, ham_test = train_test_split(ham_files, test_size=0.3, random_state=0)\n",
    "spam_ham_train, spam_ham_test = train_test_split(spam_ham_files, test_size=0.3, random_state=0)\n",
    "\n",
    "#Method which read all files of a dataframe\n",
    "def readFiles(dataFrame):\n",
    "    for i in range(len(dataFrame))\n",
    "        filename = str(ham_train.iloc[i][0])\n",
    "        with open(filename) as f:\n",
    "            contents = f.read()\n",
    "            print(contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From fork-admin@xent.com  Mon Sep 23 23:01:14 2002\n",
      "Return-Path: <fork-admin@xent.com>\n",
      "Delivered-To: yyyy@localhost.example.com\n",
      "Received: from localhost (jalapeno [127.0.0.1])\n",
      "\tby jmason.org (Postfix) with ESMTP id 2610C16F03\n",
      "\tfor <jm@localhost>; Mon, 23 Sep 2002 23:01:12 +0100 (IST)\n",
      "Received: from jalapeno [127.0.0.1]\n",
      "\tby localhost with IMAP (fetchmail-5.9.0)\n",
      "\tfor jm@localhost (single-drop); Mon, 23 Sep 2002 23:01:12 +0100 (IST)\n",
      "Received: from xent.com ([64.161.22.236]) by dogma.slashnull.org\n",
      "    (8.11.6/8.11.6) with ESMTP id g8NLnpC03988 for <jm@jmason.org>;\n",
      "    Mon, 23 Sep 2002 22:49:57 +0100\n",
      "Received: from lair.xent.com (localhost [127.0.0.1]) by xent.com (Postfix)\n",
      "    with ESMTP id 636B02941DA; Mon, 23 Sep 2002 14:42:08 -0700 (PDT)\n",
      "Delivered-To: fork@example.com\n",
      "Received: from 192.168.1.2 (smtp.piercelaw.edu [216.204.12.219]) by\n",
      "    xent.com (Postfix) with ESMTP id 8C5E72941D8; Mon, 23 Sep 2002 14:41:37\n",
      "    -0700 (PDT)\n",
      "Received: from 192.168.30.220 ([192.168.30.220]) by 192.168.1.2;\n",
      "    Mon, 23 Sep 2002 17:45:03 -0400\n",
      "From: bitbitch@magnesium.net\n",
      "X-Mailer: The Bat! (v1.61) Educational\n",
      "Reply-To: bitbitch@magnesium.net\n",
      "X-Priority: 3 (Normal)\n",
      "Message-Id: <13434889868.20020923174444@magnesium.net>\n",
      "To: fork-admin@xent.com, \"John Hall\" <johnhall@evergo.net>\n",
      "Cc: \"FoRK\" <fork@example.com>\n",
      "Subject: Re[2]: Goodbye Global Warming\n",
      "In-Reply-To: <000c01c26347$730dd770$0200a8c0@JMHALL>\n",
      "References: <000c01c26347$730dd770$0200a8c0@JMHALL>\n",
      "MIME-Version: 1.0\n",
      "Content-Type: text/plain; charset=us-ascii\n",
      "Content-Transfer-Encoding: 7bit\n",
      "Sender: fork-admin@xent.com\n",
      "Errors-To: fork-admin@xent.com\n",
      "X-Beenthere: fork@example.com\n",
      "X-Mailman-Version: 2.0.11\n",
      "Precedence: bulk\n",
      "List-Help: <mailto:fork-request@xent.com?subject=help>\n",
      "List-Post: <mailto:fork@example.com>\n",
      "List-Subscribe: <http://xent.com/mailman/listinfo/fork>, <mailto:fork-request@xent.com?subject=subscribe>\n",
      "List-Id: Friends of Rohit Khare <fork.xent.com>\n",
      "List-Unsubscribe: <http://xent.com/mailman/listinfo/fork>,\n",
      "    <mailto:fork-request@xent.com?subject=unsubscribe>\n",
      "List-Archive: <http://xent.com/pipermail/fork/>\n",
      "Date: Mon, 23 Sep 2002 17:44:44 -0400\n",
      "X-Spam-Status: No, hits=-1.3 required=5.0\n",
      "\ttests=IN_REP_TO,KNOWN_MAILING_LIST,NO_REAL_NAME,\n",
      "\t      QUOTED_EMAIL_TEXT,REFERENCES,REPLY_WITH_QUOTES,\n",
      "\t      SIGNATURE_SHORT_DENSE,US_DOLLARS_2,US_DOLLARS_4\n",
      "\tversion=2.50-cvs\n",
      "X-Spam-Level: \n",
      "\n",
      "\n",
      "\n",
      "How about this:   A bored FoRKer said:  \"YAWN\"\n",
      "\n",
      "I believe Tom had it right.   Signal not noise.\n",
      "\n",
      "I'll start:\n",
      "\n",
      "\n",
      "              Los Angeles Times September 23, 2002 Monday\n",
      "                   Copyright 2002 / Los Angeles Times\n",
      "                            Los Angeles Times\n",
      "\n",
      "September 23, 2002 Monday  Home Edition\n",
      "SECTION: Main News Main News; Part 1; Page 13; National Desk\n",
      "LENGTH: 1013 words\n",
      "BYLINE: DANA CALVO, TIMES STAFF WRITER\n",
      "DATELINE: SEATTLE\n",
      "\n",
      "BODY:\n",
      "\n",
      "The idea came at the end of a long, frustrating brown-bag session at a\n",
      "public-policy think tank here.\n",
      "\n",
      "The challenge was to save the city's child-care programs. Staring into\n",
      "his empty coffee cup, the meeting coordinator's mind landed on an\n",
      "unlikely solution: Put a tax--just a \"benign\" dime a shot--on\n",
      "espresso.\n",
      "\n",
      "That led to a petition signed by more than 20,000 Seattle residents,\n",
      "and next year, voters will decide whether the tax becomes law, one\n",
      "that taps right into Seattle's legendary addiction to coffee. This is,\n",
      "after all, the town where Starbucks was born and where the $12 pound\n",
      "of beans became a staple. There is one Starbucks for every 7,000\n",
      "residents in Seattle, compared to one per 64,000 in New York. Seattle\n",
      "also has two other major coffee chains, Tully's Coffee and Seattle's\n",
      "Best Coffee, as well as countless cafes and espresso carts.\n",
      "\n",
      "A recent poll showed that 74% of Seattle residents would vote for the\n",
      "tax. \"For people outside of Seattle who don't understand the\n",
      "consumption of espresso, [the tax proposal] can be seen as crazy,\"\n",
      "said John Burbank, the think tank's executive director, \"but it was\n",
      "common sense.\"\n",
      "\n",
      "Research by his nonprofit Economic Opportunity Institute showed that\n",
      "people preferred a tax on liquor or beer over one on espresso. But\n",
      "because of the large number of lattes and cappuccinos sold, a tax on\n",
      "espresso could be lower than one levied on alcohol.\n",
      "\n",
      "Burbank estimates the tax could generate $7 million to $10 million a\n",
      "year. City Council aides dispute his figures, saying their research\n",
      "shows the tax would bring in $1.5 million to $3 million a year.\n",
      "\n",
      "Burbank's institute is funded by foundations and labor unions. The\n",
      "think tank's mission is to promote public policy in the interests of\n",
      "low-income people, and it has long championed child-care issues.\n",
      "\n",
      "Burbank says the tax would restore cuts to the child-care programs\n",
      "made earlier this year by Gov. Gary Locke. He also says it would\n",
      "provide more low-income families with subsidies for child care,\n",
      "improve preschool programs and increase teacher salaries.\n",
      "\n",
      "At Bauhaus Books & Coffee, the sidewalk is dotted with tables of\n",
      "customers for whom coffee is a half-day activity, not just a drink.\n",
      "\n",
      "Espresso lovers like Chris Altman, who at a dime a day would spend an\n",
      "extra $36.50 a year, said the investment is worth it. \"I'm OK with\n",
      "it,\" said the 35-year-old, stirring his iced latte. \"The money's got\n",
      "to come from somewhere.\"\n",
      "\n",
      "Hope Revuelto, 25, was cooling her regular coffee ($1 because she\n",
      "brought her own mug) and reading \"Zen and the Art of Pottery.\" She\n",
      "supports the initiative and said its critics are behaving as would be\n",
      "expected of espresso drinkers: They want the most expensive thing on\n",
      "the menu but resist paying 10 cents to help the needy.\n",
      "\n",
      "Some say the tax isn't the issue; they just resent being singled out.\n",
      "\n",
      "David Marsh, 45, a costume manager, drinks up to three espressos a\n",
      "day, which means he'd be shelling out an extra $109.50 a year. \"I, for\n",
      "one, don't have kids, but I drink espresso,\" he said, as he sewed a\n",
      "leather collar onto a chain-mail tunic. \"I don't mind paying, but I\n",
      "think everyone should pay.\"\n",
      "\n",
      "Coffeehouses are steamed about it, and they've organized as\n",
      "JOLT--Joined to Oppose the Latte Tax. Among the members are Seattle's\n",
      "Chamber of Commerce and the city's two largest coffee franchises,\n",
      "Starbucks and Tully's.\n",
      "\n",
      "The tax would force coffeehouses to track sales of any beverage that\n",
      "contains espresso, a task that could be an administrative nightmare\n",
      "for smaller cafes, especially during the frantic morning rush. If\n",
      "espresso counts come under suspicion, coffeehouse owners could face a\n",
      "city audit.\n",
      "\n",
      "University of Chicago economics professor Michael Greenstone said the tax doesn't add up. \n",
      "\n",
      "\"The purpose of any tax is to be efficient and equitable, and this is\n",
      "neither,\" he said. \"On the efficiency side, it's surely going to lead\n",
      "to costly efforts by both businesses and consumers to find ways to\n",
      "avoid the tax. For example, Starbucks could claim that they are using\n",
      "finely ground coffee, [instead of coffee run though an espresso maker]\n",
      "and that consequently, they are exempt from the tax. Would they be\n",
      "right? I don't know, but finding out will surely take lots of legal\n",
      "fees that could have gone to child care.\n",
      "\n",
      "\"Of course, from a public-relations perspective, this is an ingenious\n",
      "idea, and I mean that in a cynical way. They've pitted espresso\n",
      "drinkers against child-care supporters, and who's going to side with\n",
      "the espresso drinkers?\"\n",
      "\n",
      "In fact, the proposed tax has forced opponents into a political\n",
      "two-step, where their criticism must remain a beat behind their public\n",
      "stance of political correctness. In a liberal city like Seattle,\n",
      "corporations continually advertise their commitment to social\n",
      "activism, and throughout the debate over the initiative, JOLT members\n",
      "prefaced their opposition with endorsements of good child care.\n",
      "\n",
      "\"Starbucks will continue to support early-learning and\n",
      "childhood-development programs through the millions of dollars we\n",
      "contribute annually,\" the company said. \"However, Starbucks does not\n",
      "understand why the Economic Opportunity Institute would recommend an\n",
      "additional consumer tax on espresso beverages, or any other single\n",
      "consumer product.\"\n",
      "\n",
      "The City Council has yet to decide when the initiative will go before\n",
      "voters next year. The initiative's authors say it is directed at\n",
      "vendors; critics predict it will be passed on to consumers through\n",
      "higher prices, effectively punishing them for their choice of coffee.\n",
      "The tax would be applied to any drink with at least half an ounce of\n",
      "espresso, including decaf. Drip coffee would be exempt.\n",
      "\n",
      "Burbank says the tax would reach only a pre-selected group of\n",
      "consumers who are wealthier than those who drink drip. So, he's been\n",
      "pitching it as a modern-day Robin Hood tax, where the needy get a dime\n",
      "every time the affluent spend $3 to $4 on an espresso.\n",
      "\n",
      "It's the kind of political marketing that Fran Beulah, 43, finds\n",
      "funny. \"I drink espresso,\" she said, laughing, \"and I am not rich.\"\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "JH> \"I did not have sex with that woman.\"\n",
      "\n",
      ">> -----Original Message-----\n",
      ">> From: fork-admin@xent.com [mailto:fork-admin@xent.com] On Behalf Of\n",
      "JH> Mr.\n",
      ">> FoRK\n",
      ">> Sent: Monday, September 23, 2002 2:12 PM\n",
      ">> To: FoRK\n",
      ">> Subject: Re: Goodbye Global Warming\n",
      ">> \n",
      ">> \n",
      ">> ----- Original Message -----\n",
      ">> From: \"John Hall\" <johnhall@evergo.net>\n",
      ">> \n",
      ">> > A Green once said that if the Spotted Owl hadn't existed they\n",
      ">> > would have had to invent it.\n",
      ">> A Republican once said \"I am not a crook\".\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "-- \n",
      "Best regards,\n",
      " bitbitch                            mailto:bitbitch@magnesium.net\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "readFiles(ham_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 - Write a Python program that:\n",
    "#### a. Uses four datasets (hamtrain, spamtrain, hamtest, and spamtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. Using a Naïve Bayes classifier (e.g. Sklearn), classifies the test sets and reports the  percentage of ham and spam test sets that were classified correctly. You can use CountVectorizer to transform the email texts into vectors. Please note that there are different types of Naïve Bayes Classifier in SKlearn (Document is available here). Test two of these classifiers: 1. Multinomial Naive Bayes and 2. Bernoulli Naive Bayes that are well suited for this problem. For the case of Bernoulli Naive Bayes you should use the parameter binarize to make the features binary. Discuss the differences between these two classifiers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3 - Run your program on\n",
    "#### i. Spam versus easy-ham"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ii. Spam versus hard-ham"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4 - To avoid classification based on common and uninformative words it is common to filter these out. \n",
    "#### a. Argue why this may be useful. Try finding the words that are too common/uncommon in the dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. Use the parameters in Sklearn’s CountVectorizer to filter out these words. Run the updated program on your data and record how the results differ from 3. You have two options to do this in Sklearn: either using the words found in part (a) or letting Sklearn do it for you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5 - Filter out the headers and the footers of the emails before you run on them. The format may vary somewhat between emails, which can make this a bit tricky, so perfect filtering is not required. Run your program again and answer the following questions: \n",
    "#### a. Does the result improve from 3 and 4?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. The split of the data set into a training set and a test set can lead to very skewed results. Why is this, and do you have suggestions on remedies?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c. What do you expect would happen if your training set were mostly spam messages while your test set were mostly ham messages?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
